"""
创建 WebDriver对象，指明使用Edge浏览器驱动。可以选择在Edge中传入ChromiumDriver对象
然后实例化一个Webdriver对象wd
自己寻找合适的视频，然后点开评论区，右键检查第一条评论，当【高亮部分】盖过【作者名】以及【展开...条评论】时
点击复制xpath路径，注意千万【不要】复制完整xpath路径,完整路径很容易变化，找不到指定元素就崩溃了
最后快速将【鼠标移动到评论区】，等待开启的另外一个线程自动下拉刷新评论，然后就不要再动了！
我这里设置的评论刷新速度并不是非常快，见第121行，主要是刷新过快会导致浏览器占用内存激增很容易崩溃
为了避免这种情况特地降低了评论刷新速度
"""
from selenium import webdriver
from selenium.webdriver.common.by import By
from tqdm import tqdm
from time import sleep
import threading
import pyautogui
import json
import re


def login():
    """
    请注意，抖音反爬挺厉害了，你用Cookie登录会【经常】性的跳出验证码，未来还是需要对这一块作出自动化处理
    """
    with open(r"C:/Users/潘炳隆/Desktop/load_data.json", 'r') as f:
        cookies_file = f.read()

    cookie_list = json.loads(cookies_file)          # 将读取的文件转为json格式
    web_driver = webdriver.Edge()
    web_driver.get('https://www.douyin.com/')

    for cookie in cookie_list:
        web_driver.add_cookie(cookie)                  # 遍历之前获取到的Cookie字典的列表并添加到网页中

    web_driver.refresh()                              # 需要刷新网页才能登录
    web_driver.get('https://www.douyin.com/')        # 再输入一次网址即可显示登录成功
    web_driver.maximize_window()                    # 最大化窗口
    return web_driver


def load_basic_data():
    input('先登录，并找到评论信息元素...')
    input_xpath = input('请输入Xpath路径：')
    sleep(5)     # 等待其他程序启动刷新数据
    return input_xpath


def deal_xpath(_xpath):
    """
    :param _xpath: 手动复制的Xpath相对路径
    :return:      从右往左帮你去掉第一个[1]
    """
    _xpath = list(_xpath)
    local_index = 0
    for c in _xpath[::-1]:
        local_index -= 1
        if c == '1':
            break
    _xpath[local_index - 1:local_index + 2] = ''
    dealt_xpath = ''.join(_xpath)
    return dealt_xpath


def analysis_data(start, web_elements_data, original_data_list):
    """
    :param start: 起始索引
    :param web_elements_data: 待处理数据
    :param original_data_list: 装有之前处理过的数据列表
    :return: 处理完数据的列表
    """
    # 所看即所得，完全基于正则提取。下面对于抖音评论信息的正则提取可谓大费精力！
    global error_info
    max_count = len(web_elements_data)
    if start == max_count:
        sleep(1)         # 避免一直让程序高负荷运行，休息一会，等数据刷新
        return original_data_list, max_count, False
    for index in tqdm(range(start, max_count)):
        print('index=', index)
        try:
            raw_data = web_elements_data[index].text
            data = raw_data.replace('\n', '##···', 1).replace('\n', '')
            pattern = r'^(.*?)(作者)?##···(.*?)(置顶|作者赞过|作者回复过)?\d{1,3}(分钟|小时|天|周|月|年)前·(.+?)(\d.*?)分享回复(展开(\d+)条回复)?'
            match_result = re.findall(pattern, data)[0]
            username = match_result[0]      # 用户名
            comment = match_result[2]       # 评论
            like = match_result[-3]         # 点赞
            ip = match_result[-4]           # ip地址
            reply_count = int(match_result[-1]) if match_result[-1] != '' else 0
        except (IndexError, re.error)as error:
            error_info.append(f'错误信息：第{index}条数据获取失败，失败原因：{error}')
            continue
        json_dic = dict()
        json_dic['用户名'] = username
        json_dic['评论'] = comment
        json_dic['点赞数'] = like
        json_dic['回复评论数'] = reply_count
        json_dic['ip属地'] = ip
        original_data_list.append(json_dic)
    print(f'已成功写入数据：{len(original_data_list)}条！！')
    return original_data_list, max_count, True


def save_data(wait_for_save: list):         # 保存数据
    content = json.dumps(wait_for_save, indent=4, ensure_ascii=False)
    with open("C:/Users/潘炳隆/Desktop/抖音评论数据.json", 'w', encoding='utf-8')as file:
        file.write(content)


def main(_xpath, _lst, _original_end):
    # original_end: 原先结束的位置
    global wd
    new_web_elements = wd.find_elements(By.XPATH, _xpath)                        # 定位到的所有评论信息
    main_data = analysis_data(_original_end, new_web_elements, _lst)             # 解析数据
    waiting_for_save = main_data[0]
    last_location = main_data[1]
    if main_data[2]:       # 有新评论才写入文件
        save_data(waiting_for_save)                                                  # 保存数据
    return waiting_for_save, last_location


def refresh_comments():
    global if_refresh_comments_stop
    sleep(5)       # 无限下拉前先给他睡个五秒
    while not if_refresh_comments_stop:
        pyautogui.scroll(-600)
        sleep(0.1)


if __name__ == '__main__':
    # 第一步，初始化基本数据：
    if_refresh_comments_stop = False
    lst = []                       # lst是保存所有评论数据的列表
    error_info = []                # 错误日志列表，在程序异常终止时打印所有的错误信息
    original_end = 0               # 记录上一次抓取到第几条评论
    # 第二步：携带Cookie登录网址
    wd = login()
    # 第三步，载入必要数据
    raw_xpath = load_basic_data()               # 获取你复制的xpath路径
    search_window = wd.current_window_handle   # 切换句柄
    xpath = deal_xpath(raw_xpath)             # 处理输入的xpath路径，从右往左去掉第一个[1],前提是你检查的是视频的第一条评论
    thread = threading.Thread(target=refresh_comments)    # 开启一个新的线程，启动自动下拉脚本
    thread.start()
    # 第三步，持续化操作解析并写入评论数据
    # 循环逻辑：while->main()->analysis_data()->save_data->main()->while
    try:
        while True:
            x = main(xpath, lst, original_end)     # 调用主函数
            lst = x[0]
            original_end = x[1]
    except Exception as e:               # 手动结束进程可以选择自己关闭浏览器；
        wd.quit()
        if_refresh_comments_stop = True
        thread.join(3)
        print('所有的错误信息：', '\n'.join(error_info))
        input(f'程序终止！！错误原因:{e}')
